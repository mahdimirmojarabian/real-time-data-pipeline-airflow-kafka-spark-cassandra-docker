# ğŸ”— Real-Time Data Streaming & Processing Pipeline â€” Powered by Airflow, Kafka, Spark, Cassandra, and Docker

âœ¨ **Tools & Technologies Used:**

* ğŸª‚ **Apache Airflow**: Workflow orchestration
* ğŸ˜ **Apache Kafka**: Real-time data streaming
* ğŸ”¥ **Apache Spark (Structured Streaming)**: Data processing and transformation
* ğŸ—ƒï¸ **Apache Cassandra**: NoSQL database for storage
* ğŸ³ **Docker & Docker Compose**: Containerization for seamless deployment
* ğŸ **Python**: Scripting and data processing
* ğŸŒ **REST API**: Source of random name data

---

## ğŸ¯ Project Overview

This project is a fully containerized, real-time data streaming and processing pipeline. It periodically fetches random user data from an external API, streams it through Kafka, processes it using Spark, and stores it in a Cassandra database. Docker is used to deploy all components effortlessly and maintain isolated environments.

### ğŸ–¥ï¸ Architecture Diagram

```mermaid
graph TD
subgraph Docker
API[ğŸŒ Random Name API] --> Airflow[ğŸª‚ Airflow]
Airflow --> Kafka[ğŸ˜ Kafka]
Kafka --> Spark[ğŸ”¥ Spark]
Spark --> Cassandra[ğŸ—ƒï¸ Cassandra]
end
```

---

## ğŸ“Œ How It Works

1. **ğŸª‚ Apache Airflow** triggers a script every 10 seconds to fetch data from an external random names API.
2. The data is then published to a **ğŸ˜ Apache Kafka** topic (`random_names`).
3. **ğŸ”¥ Apache Spark** (Structured Streaming) consumes messages from Kafka, processes them, and writes the results to **ğŸ—ƒï¸ Apache Cassandra**.

---

## ğŸ“‚ Project Files

* `stream_to_kafka_dag.py`: ğŸª‚ Airflow DAG script for scheduling API data fetch every 10 seconds.
* `stream_to_kafka.py`: ğŸŒ Fetches data from the API and sends it to ğŸ˜ Kafka.
* `spark_streaming.py`: ğŸ”¥ Consumes data from ğŸ˜ Kafka with Spark and writes to ğŸ—ƒï¸ Cassandra.
* `response.json`: ğŸŒ Sample API response.

---

## âš™ï¸ Setup and Installation

### ğŸ³ Step 1: Clone the Repository

```bash
git clone <repository_url>
```

### ğŸ‹ Step 2: Docker Configuration

#### ğŸ“Œ Build Airflow Docker Container

Run once to build your customized Airflow container:

```bash
docker build --rm \
  --build-arg AIRFLOW_DEPS="datadog,dask" \
  --build-arg PYTHON_DEPS="flask_oauthlib>=0.9" \
  -t puckel/docker-airflow .
```

Replace the existing `docker-compose-LocalExecutor.yml` file with the provided one, and add `requirements.txt`.

Launch the containers:

```bash
docker-compose -f docker-compose-LocalExecutor.yml up -d
```

Access Airflow UI: `http://localhost:8080`

---

### ğŸ“¨ Apache Kafka Setup

Launch Kafka cluster and Cassandra:

```bash
docker-compose up -d
```

Access Kafka UI: `http://localhost:8888`

Create Kafka topic:

* Topic name: `random_names`

---

### ğŸ—ƒï¸ Apache Cassandra Setup

Access Cassandra CLI:

```bash
docker exec -it cassandra /bin/bash
cqlsh -u cassandra -p cassandra
```

Create Cassandra keyspace and table:

```sql
CREATE KEYSPACE spark_streaming WITH replication = {'class':'SimpleStrategy','replication_factor':1};

CREATE TABLE spark_streaming.random_names(
  full_name text PRIMARY KEY,
  gender text,
  location text,
  city text,
  country text,
  postcode int,
  latitude float,
  longitude float,
  email text
);
```

---

### â–¶ï¸ Running the Data Pipeline

#### ğŸ“‚ Airflow DAGs

Place the files `stream_to_kafka.py` and `stream_to_kafka_dag.py` inside Airflow's `dags` folder.

Activate the DAG `random_people_names` in Airflow's web UI (`http://localhost:8080`). Data will begin streaming every 10 seconds.

#### ğŸ”¥ Spark Streaming

Copy Spark script to container:

```bash
docker cp spark_streaming.py spark_master:/opt/bitnami/spark/
```

Enter the Spark container and install required JAR files:

```bash
docker exec -it spark_master /bin/bash
cd jars
curl -O https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.3.0/spark-cassandra-connector_2.12-3.3.0.jar
curl -O https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.3.0/spark-sql-kafka-0-10_2.13-3.3.0.jar
```

Submit Spark streaming job:

```bash
spark-submit --master local[2] \
  --jars /opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.13-3.3.0.jar,/opt/bitnami/spark/jars/spark-cassandra-connector_2.12-3.3.0.jar \
  spark_streaming.py
```

Data from Kafka will now be continuously written to Cassandra.

---

## ğŸ¨ Pipeline Flow Diagram

```mermaid
graph TD
subgraph Docker
subgraph Airflow
A1[ğŸŒ Fetch API Data Every 10 sec] --> A2[ğŸ˜ Send Data to Kafka]
end
subgraph Kafka Cluster
K[ğŸ˜ Kafka Topic: random_names]
end
subgraph Spark
S[ğŸ”¥ Consume from Kafka & Process Data] --> C[ğŸ—ƒï¸ Write Data to Cassandra]
end
end

API --> A1
A2 --> K
K --> S
S --> C
```
